{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
=======
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Logan/anaconda/envs/dev/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
<<<<<<< HEAD
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
=======
    "from sklearn import metrics"
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 5,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "X = pd.read_csv(\"../../data/binarized_data.csv\")\n",
    "ahi = X['ahi']\n",
    "y = X['ahi'] >= 5\n",
    "X = X.drop('ahi', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1208eb510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAECCAYAAADuGCyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1206d6550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.plt.scatter(X.age, X.bmi)"
=======
    "# Creating a clean data file; file now \"../data/binarized_data.csv\"\n",
    "\n",
    "data = pd.read_excel(\"/Users/Logan/Downloads/UMD Sleep Apnea data.xls\")\n",
    "data['gt5'] = data.ahi > 5\n",
    "\n",
    "# Fix all dummy data\n",
    "X = data.copy().iloc[:,:9]\n",
    "\n",
    "gender = pd.get_dummies(X.gender).iloc[:,0] # choose just one out of two columns\n",
    "\n",
    "ethnicity = pd.get_dummies(X.ethnicity).iloc[:,:-1] # choose every column but the last, due to collinearity\n",
    "\n",
    "allergies = pd.DataFrame(pd.get_dummies(X.allergies).iloc[:,0])\n",
    "allergies.columns = ['allergies']\n",
    "\n",
    "asthma = pd.DataFrame(pd.get_dummies(X.asthma).iloc[:,0])\n",
    "asthma.columns = ['asthma']\n",
    "\n",
    "gerd = pd.DataFrame(pd.get_dummies(X.gerd).iloc[:,0])\n",
    "gerd.columns = ['gerd']\n",
    "\n",
    "tonsilsize = pd.get_dummies(X.tonsilsize).iloc[:,:-1]\n",
    "tonsilsize.columns = ['tsize1', 'tsize2', 'tsize3']\n",
    "\n",
    "new_X = pd.concat((gender, ethnicity, allergies, asthma, gerd, tonsilsize), axis=1) # form final dataset\n",
    "X = pd.concat((data[['bmi', 'age', 'term', 'gt5']], new_X), axis=1) # add in numeric data\n",
    "\n",
    "X = X.dropna() # drop null data\n",
    "\n",
    "y = X['gt5'] # set ahi > 5 as y\n",
    "X = X.drop('gt5', axis=1) # remove ahi > 5 from input data"
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metric Functions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 6,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sensitivity(predicted_values, actual_values, rounding=False):\n",
    "    if not isinstance(predicted_values, np.ndarray):\n",
    "        predicted_values = np.array(predicted_values)\n",
    "    if not isinstance(actual_values, np.ndarray):\n",
    "        actual_values = np.array(actual_values)\n",
    "    positives = (actual_values == True)\n",
    "    found_positives = predicted_values[positives]\n",
    "    sensitivity = np.mean(found_positives == True)\n",
    "    if rounding:\n",
    "        sensitivity = round(sensitivity, rounding)\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 7,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specificity(predicted_values, actual_values, rounding=False):\n",
    "    if not isinstance(predicted_values, np.ndarray):\n",
    "        predicted_values = np.array(predicted_values)\n",
    "    if not isinstance(actual_values, np.ndarray):\n",
    "        actual_values = np.array(actual_values)\n",
    "    negatives = (actual_values == False)\n",
    "    found_negatives = predicted_values[negatives]\n",
    "    sensitivity = np.mean(found_negatives == False)\n",
    "    if rounding:\n",
    "        sensitivity = round(sensitivity, rounding)\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 8,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity, Specificity: (0.8, 0.67)\n",
      "Should be (0.8, 0.67)\n"
     ]
    }
   ],
   "source": [
    "predicted_values = [1, 0, 1, 1, 1, 0, 0, 1]\n",
    "actuality_values = [1, 1, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "# actual sensitivity / recall should be 80%\n",
    "# actual specificity should be 66%\n",
    "\n",
    "sens = round(sensitivity(predicted_values, actuality_values), 2)\n",
    "spec = round(specificity(predicted_values, actuality_values), 2)\n",
    "\n",
    "print \"Sensitivity, Specificity: ({}, {})\".format(sens, spec)\n",
    "print \"Should be (0.8, 0.67)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we have balanced classes?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 9,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.40789473684210525"
      ]
     },
     "execution_count": 20,
=======
       "0.41002277904328016"
      ]
     },
     "execution_count": 9,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 41% yes, 59% no sleep apnea > 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Kang et al.__: AHI > 5: Sensitivity: 77.5%, Specificity: 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 10,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Best Model Choice"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = np.array([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 6],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l > 1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBestSensitivity(y_pred, y_test):\n",
    "    pairs = []\n",
    "    for threshold in np.linspace(0, 1, num = 50):\n",
    "        thresholded = y_pred > threshold\n",
    "        sens = sensitivity(y_pred, y_test)\n",
    "        spec = specificity(y_pred, y_test)\n",
    "        pairs.append([threshold, sensitivity, specificity])\n",
    "    pairs = np.array(pairs)\n",
    "    adequate_spec = (pairs > 0.5)[:,2]\n",
    "    if len(adequate_spec): # if there is a matching w/ specificity > 0.5\n",
    "        best_threshold = subset = pairs[adequate_spec][0] # find first entry; has highest specificity\n",
    "    return best_threshold\n",
    "\n",
    "def naiveBestModel(i, model, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    best_sensitivity = 0\n",
    "    best_model = None\n",
    "    best_predictions = None\n",
    "    for i in range(i):\n",
    "        print i\n",
    "        m = model(**kwargs)\n",
    "        m.fit(X_train, y_train)\n",
    "        predictions = m.predict_proba(X_test)\n",
    "        best_s\n",
    "        model_sensitivity = sensitivity(predictions, y_test)\n",
    "        if model_sensitivity > best_sensitivity:\n",
    "            best_sensitivity = model_sensitivity\n",
    "            best_model = m\n",
    "            best_predictions = predictions\n",
    "    return best_model, best_predictions, y_test"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 69,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f6da4c69808e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                    criterion='entropy')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-44eb908a9b9a>\u001b[0m in \u001b[0;36mnaiveBestModel\u001b[0;34m(i, model, X_train, X_test, y_train, y_test, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbest_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Logan/anaconda/envs/dev/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Logan/anaconda/envs/dev/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Logan/anaconda/envs/dev/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
=======
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
     ]
    }
   ],
   "source": [
    "m, y_pred, y_test = naiveBestModel(1000,\n",
    "                                   RandomForestClassifier,\n",
    "                                   X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.87878787878787878, 0.036363636363636362)\n",
      "(0.87878787878787878, 0.036363636363636362)\n",
      "(0.87878787878787878, 0.036363636363636362)\n",
      "(0.87878787878787878, 0.036363636363636362)\n",
      "(0.87878787878787878, 0.036363636363636362)\n",
      "(0.78787878787878785, 0.20000000000000001)\n",
      "(0.78787878787878785, 0.20000000000000001)\n",
      "(0.78787878787878785, 0.20000000000000001)\n",
      "(0.78787878787878785, 0.20000000000000001)\n",
      "(0.78787878787878785, 0.20000000000000001)\n",
      "(0.78787878787878785, 0.29090909090909089)\n",
      "(0.78787878787878785, 0.29090909090909089)\n",
      "(0.78787878787878785, 0.29090909090909089)\n",
      "(0.78787878787878785, 0.29090909090909089)\n",
      "(0.78787878787878785, 0.29090909090909089)\n",
      "(0.72727272727272729, 0.52727272727272723)\n",
      "(0.72727272727272729, 0.52727272727272723)\n",
      "(0.72727272727272729, 0.52727272727272723)\n",
      "(0.72727272727272729, 0.52727272727272723)\n",
      "(0.72727272727272729, 0.52727272727272723)\n",
      "(0.63636363636363635, 0.69090909090909092)\n",
      "(0.63636363636363635, 0.69090909090909092)\n",
      "(0.63636363636363635, 0.69090909090909092)\n",
      "(0.63636363636363635, 0.69090909090909092)\n",
      "(0.63636363636363635, 0.69090909090909092)\n",
      "(0.54545454545454541, 0.80000000000000004)\n",
      "(0.54545454545454541, 0.80000000000000004)\n",
      "(0.54545454545454541, 0.80000000000000004)\n",
      "(0.54545454545454541, 0.80000000000000004)\n",
      "(0.54545454545454541, 0.80000000000000004)\n",
      "(0.24242424242424243, 0.8545454545454545)\n",
      "(0.24242424242424243, 0.8545454545454545)\n",
      "(0.24242424242424243, 0.8545454545454545)\n",
      "(0.24242424242424243, 0.8545454545454545)\n",
      "(0.24242424242424243, 0.8545454545454545)\n",
      "(0.090909090909090912, 0.92727272727272725)\n",
      "(0.090909090909090912, 0.92727272727272725)\n",
      "(0.090909090909090912, 0.92727272727272725)\n",
      "(0.090909090909090912, 0.92727272727272725)\n",
      "(0.090909090909090912, 0.92727272727272725)\n",
      "(0.030303030303030304, 1.0)\n",
      "(0.030303030303030304, 1.0)\n",
      "(0.030303030303030304, 1.0)\n",
      "(0.030303030303030304, 1.0)\n",
      "(0.030303030303030304, 1.0)\n",
      "(0.0, 1.0)\n",
      "(0.0, 1.0)\n",
      "(0.0, 1.0)\n",
      "(0.0, 1.0)\n",
      "(0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "probabilities = m.predict_proba(X_test)[:,1]\n",
    "for classification_threshold in np.linspace(0, 1, num = 50):\n",
    "    y_pred = probabilities > classification_threshold\n",
    "    print (sensitivity(y_pred, y_test), specificity(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      0.80      0.77        55\n",
      "       True       0.62      0.55      0.58        33\n",
      "\n",
      "avg / total       0.70      0.70      0.70        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "best_sensitivity = 0\n",
    "best_model = None\n",
    "best_predictions = predictions\n",
    "for i in range(100):\n",
    "    print i\n",
    "    m = RandomForestClassifier(n_estimators = 30, criterion='gini', bootstrap=False)\n",
    "    m.fit(X_train, y_train)\n",
    "    predictions = m.predict(X_test)\n",
    "    model_sensitivity = sensitivity(predictions, y_test)\n",
    "    if model_sensitivity > best_sensitivity:\n",
    "        best_sensitivity = model_sensitivity\n",
    "        best_model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.69      0.64      0.66        55\n",
      "       True       0.46      0.52      0.49        33\n",
      "\n",
      "avg / total       0.60      0.59      0.59        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, best_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = DecisionTreeClassifier(criterion='entropy')\n",
    "m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity(predictions, y_test, rounding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity(predictions, y_test, rounding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.69      0.64      0.66        55\n",
      "       True       0.46      0.52      0.49        33\n",
      "\n",
      "avg / total       0.60      0.59      0.59        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy'))\n",
    "m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.64      0.77      0.70        47\n",
      "       True       0.66      0.51      0.58        41\n",
      "\n",
      "avg / total       0.65      0.65      0.64        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = LogisticRegression()\n",
    "m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 185,
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.53      0.87      0.66        47\n",
      "       True       0.45      0.12      0.19        41\n",
      "\n",
      "avg / total       0.50      0.52      0.44        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = SVC()\n",
    "m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LASSO Logistic on Feature Engineered Data"
   ]
  },
  {
=======
>>>>>>> 66605738dbeacabaf34c59a8a0f50db45001bc46
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
